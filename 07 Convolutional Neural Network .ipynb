{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/wd/data/MNIST/\"\n",
    "data = input_data.read_data_sets(DATA_DIR, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 379760\r\n",
      "-rw-r--r--@ 1 abulbasar  staff   18289443 May  2 19:47 mnist_test.csv\r\n",
      "-rw-r--r--@ 1 abulbasar  staff  109575994 May  2 19:48 mnist_train.csv\r\n",
      "-rw-r--r--  1 abulbasar  staff    7840016 Apr  7 18:48 t10k-images-idx3-ubyte\r\n",
      "-rw-r--r--  1 abulbasar  staff    1648877 Apr 30 12:49 t10k-images-idx3-ubyte.gz\r\n",
      "-rw-r--r--  1 abulbasar  staff      10008 Apr  7 18:48 t10k-labels-idx1-ubyte\r\n",
      "-rw-r--r--  1 abulbasar  staff       4542 Apr 30 12:49 t10k-labels-idx1-ubyte.gz\r\n",
      "-rw-r--r--  1 abulbasar  staff   47040016 Apr  7 18:48 train-images-idx3-ubyte\r\n",
      "-rw-r--r--  1 abulbasar  staff    9912422 Apr 30 12:49 train-images-idx3-ubyte.gz\r\n",
      "-rw-r--r--  1 abulbasar  staff      60008 Apr  7 18:48 train-labels-idx1-ubyte\r\n",
      "-rw-r--r--  1 abulbasar  staff      28881 Apr 30 12:49 train-labels-idx1-ubyte.gz\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l ../data/MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x:  (?, 784)\n",
      "Shape of y_true:  (?, 10)\n",
      "Shape of y_pred:  (?, 10)\n",
      "Accuracy: 91.5%\n"
     ]
    }
   ],
   "source": [
    "NUM_STEPS = 1000\n",
    "MINIBATCH_SIZE = 100\n",
    "learnin_rate = 0.2\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "print(\"Shape of x: \", x.get_shape())\n",
    "print(\"Shape of y_true: \", y_true.get_shape())\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "bias = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "y_pred = tf.matmul(x, W) + bias\n",
    "print(\"Shape of y_pred: \", y_pred.get_shape())\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= y_true, logits=y_pred))\n",
    "\n",
    "train = tf.train.GradientDescentOptimizer(learnin_rate).minimize(cross_entropy)\n",
    "#train = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_mask = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Train\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(NUM_STEPS):\n",
    "        batch_xs, batch_ys = data.train.next_batch(MINIBATCH_SIZE)\n",
    "        sess.run(train, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "\n",
    "    # Test\n",
    "    ans = sess.run(accuracy, feed_dict={x: data.test.images, y_true: data.test.labels})\n",
    "    \n",
    "\n",
    "print(\"Accuracy: {:.4}%\".format(ans*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def conv_layer(input, shape):\n",
    "    W = weight_variable(shape)\n",
    "    b = bias_variable([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input, W) + b)\n",
    "\n",
    "\n",
    "def full_layer(input, size):\n",
    "    in_size = int(input.get_shape()[1])\n",
    "    W = weight_variable([in_size, size])\n",
    "    b = bias_variable([size])\n",
    "    return tf.matmul(input, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### weight_variable\n",
    "These are the weights for either fully connected or convolutional layers of the network. They are initialized randomly using a truncated normal distribution with a standard deviation of .1. This sort of initialization with a random normal distribution that is truncated at the tails, is pretty common and generally produces good results (see note on random initialization below).\n",
    "\n",
    "#### bias_variable \n",
    "The bias elements in either a fully connected or a convolutional layer. These are all initialized with the constant value of .1. \n",
    "\n",
    "#### conv2d \n",
    "The convolution we will typically use. A full convolution (no skips) with an output the same size as the input. \n",
    "max_pool_2x2 - max pool to half the size across the height/width dimensions, and in total quarter the size of the feature map.\n",
    "\n",
    "#### conv_layer \n",
    "This is the actual layer we will use. Linear convolution as defined in conv2d, with a bias, and followed by the relu non-linearity.\n",
    "\n",
    "#### full_layer \n",
    "A standard full layer with a bias. Notice that here we didn’t add the relu. This allows us to use the same layer for the final output where we don’t need the non-linear part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "conv1 = conv_layer(x_image, shape=[5, 5, 1, 32])\n",
    "conv1_pool = max_pool_2x2(conv1)\n",
    "\n",
    "conv2 = conv_layer(conv1_pool, shape=[5, 5, 32, 64])\n",
    "conv2_pool = max_pool_2x2(conv2)\n",
    "\n",
    "conv2_flat = tf.reshape(conv2_pool, [-1, 7*7*64])\n",
    "full_1 = tf.nn.relu(full_layer(conv2_flat, 1024))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "full1_drop = tf.nn.dropout(full_1, keep_prob=keep_prob)\n",
    "\n",
    "y_conv = full_layer(full1_drop, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.10000000149011612\n",
      "step 100, training accuracy 0.8399999737739563\n",
      "step 200, training accuracy 0.8600000143051147\n",
      "step 300, training accuracy 0.8999999761581421\n",
      "step 400, training accuracy 0.9399999976158142\n",
      "step 500, training accuracy 0.8999999761581421\n",
      "step 600, training accuracy 0.9399999976158142\n",
      "step 700, training accuracy 0.9599999785423279\n",
      "step 800, training accuracy 0.8799999952316284\n",
      "step 900, training accuracy 0.9800000190734863\n",
      "test accuracy: 0.95660001039505\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_conv, labels= y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(NUM_STEPS):\n",
    "        batch = mnist.train.next_batch(50)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x: batch[0], y_: batch[1],\n",
    "                                                           keep_prob: 1.0})\n",
    "            print(\"step {}, training accuracy {}\".format(i, train_accuracy))\n",
    "\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "    X = mnist.test.images.reshape(10, 1000, 784)\n",
    "    Y = mnist.test.labels.reshape(10, 1000, 10)\n",
    "    test_accuracy = np.mean([sess.run(accuracy, \n",
    "                             feed_dict={x:X[i], y_:Y[i],keep_prob:1.0}) \n",
    "                             for i in range(10)])\n",
    "\n",
    "print(\"test accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
